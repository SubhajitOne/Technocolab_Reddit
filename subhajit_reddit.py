# -*- coding: utf-8 -*-
"""Subhajit Reddit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xcZmYyJV3OHzZcjfbGJqITLTMa0ieZtD
"""

import nltk
import numpy as np
import pandas as pd
import tensorflow as ts
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

df_pos = pd.read_csv('/content/drive/MyDrive/Technocolab/archive/comments_positive.csv')
df_neg = pd.read_csv('/content/drive/MyDrive/Technocolab/archive/comments_negative.csv')

[df_pos.shape,df_neg.shape]

df_pos.dropna(inplace=True)
df_neg.dropna(inplace = True)

df_pos.head()

df_pos = df_pos.sample(frac = 0.05, random_state=101)
df_neg = df_neg.sample(frac = 0.05, random_state=101)

"""## Cleaning The Dataset"""

import nltk
nltk.download('stopwords')

def clean(text, stemming=False, stop_words=True):
    import re
    from string import punctuation
    from nltk.stem import SnowballStemmer
    from nltk.corpus import stopwords
    from nltk import word_tokenize
    
    
    # Empty comment
    if type(text) != str or text=='':
        return ''
    
    # Commence the cleaning!
    urls = r'http(s)*:\/\/(\w|\.)+(\/\w+)*'
    text = re.sub(urls, '', text, flags=re.IGNORECASE)
    text = re.sub("\'re", " are", text)
    text = re.sub("\'ve", " have", text)
    text = re.sub("\'d", " would", text)
    text = re.sub("cant", "can not", text)
    text = re.sub("can\'t", "can not", text)
    text = re.sub("isn\'t", "is not", text)
    text = re.sub("isnt", "is not", text)
    text = re.sub("whats", "what is", text)
    text = re.sub("what\'s", "what is", text)
    text = re.sub("shouldn't", "should not", text, flags=re.IGNORECASE)
    text = re.sub("I'm", "I am", text)
    text = re.sub(":", " ", text)
    # The comments contain \n for line breaks, we need to remove those too
    text = re.sub("\\n", " ", text)
    
    # Special characters
    text = re.sub('\&', " and ", text)
    text = re.sub('\$', " dollar ", text)
    text = re.sub('\%', " percent ", text)
    
    # Remove punctuation
    text = ''.join([word for word in text if word not in punctuation]).lower()
    
    # If we want to do stemming...
    if stemming:
        sno = SnowballStemmer('english')
        text = ''.join([sno.stem[word] for word in text])
    
    # If we want to remove stop words...
    stops = stopwords.words('english')
    if stop_words:
        text = text.split()
        text = [word for word in text if word not in stops]
        text = ' '.join(text)
    
    return text

df_pos['text'] = df_pos['text'].apply(clean).astype(str)
df_pos['parent_text'] = df_pos['parent_text'].apply(clean).astype(str)

df_neg['text'] = df_neg['text'].apply(clean).astype(str)
df_neg['parent_text'] = df_neg['parent_text'].apply(clean).astype(str)

#Concatinating the data
df = pd.concat([df_pos, df_neg])
df = df.sample(frac=1).reset_index(drop=True)
df.dropna(axis=0, inplace=True)

#combining the text and parent_text
df['combined'] = df[['text','parent_text']].apply(lambda x: ''.join(x),axis = 1)

df['combined'].shape

df_1 = df.sample(frac=0.05,random_state=101)

df_1['combined'].shape

"""## Splitting the Data"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
...     df_1['combined'], df_1['score'], test_size=0.33, random_state=101)

from sklearn.feature_extraction.text import TfidfVectorizer

tf = TfidfVectorizer()

X_train = tf.fit_transform(X_train)
X_test = tf.transform(X_test)

"""### Model"""

from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor

ln = LinearRegression()
kn = KNeighborsRegressor(n_neighbors=5)
rf = RandomForestRegressor()

ln.fit(X_train,y_train)

kn.fit(X_train,y_train)

rf.fit(X_train,y_train)

"""### Prediction"""

from sklearn.metrics import mean_squared_error as mse

pred_ln = ln.predict(X_test)

rmse_ln = (mse(y_test,pred_ln))**0.5
rmse_ln

pred_kn = kn.predict(X_test)

rmse_kn = (mse(y_test,pred_kn))**0.5
rmse_kn

pred_rf = rf.predict(X_test)

rmse_rf = (mse(y_test,pred_rf))**0.5
rmse_rf

pred = pd.DataFrame(data=[rmse_rf,rmse_ln,rmse_kn],index=['Random_Forest','Logistic_Regression','KNN'],columns=['RMSE_score'])

pred

#So we see thar rf model performs better and so we use rf model for further prediction

"""### Pickle"""

import pickle

pk = open('model.pkl','wb')
pickle.dump(ln,pk)
pickle.dump(tf, pk)

